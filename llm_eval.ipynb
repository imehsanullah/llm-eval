{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended Multi-Server Performance Comparison: GPUs vs CPUs\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#  server configurations\n",
    "servers = {\n",
    "    # \"RTX_5090\": {\n",
    "    #     \"url\": \"...\",\n",
    "    #     \"hardware\": \"RTX 5090\",\n",
    "    #     \"type\": \"GPU\",\n",
    "    #     \"memory\": \"32GB VRAM\",\n",
    "    #     \"cores\": \"N/A\"\n",
    "    # },\n",
    "    # \"RTX_4090\": {\n",
    "    #     \"url\": \"...\", \n",
    "    #     \"hardware\": \"RTX 4090\",\n",
    "    #     \"type\": \"GPU\", \n",
    "    #     \"memory\": \"24GB VRAM\",\n",
    "    #     \"cores\": \"N/A\"\n",
    "    # },\n",
    "    # \"AMD_EPYC\": {\n",
    "    #     \"url\": \"...\",\n",
    "    #     \"hardware\": \"AMD EPYC (with IBPB)\",\n",
    "    #     \"type\": \"CPU\",\n",
    "    #     \"memory\": \"System RAM\",\n",
    "    #     \"cores\": \"Multiple cores\"\n",
    "    # },\n",
    "    # \"AMD_Threadripper\": {\n",
    "    #     \"url\": \"...\",\n",
    "    #     \"hardware\": \"AMD Ryzen Threadripper 7960X 24-Cores\",\n",
    "    #     \"type\": \"CPU\",\n",
    "    #     \"memory\": \"System RAM\", \n",
    "    #     \"cores\": \"24 cores\"\n",
    "    # }\n",
    "    \n",
    "    \"A40_GPU\": {\n",
    "        \"url\": \"...\",\n",
    "        \"hardware\": \"A40 GPU\",\n",
    "        \"type\": \"GPU\",\n",
    "        \"memory\": \"48GB VRAM\",\n",
    "        \"cores\": \"N/A\"\n",
    "    },\n",
    "    \"Contabo_Server\": {\n",
    "        \"url\": \"...\",\n",
    "        \"hardware\": \"Contabo Server\",\n",
    "        \"type\": \"CPU\",\n",
    "        \"memory\": \"System RAM\",\n",
    "        \"cores\": \"Multiple cores\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test configuration\n",
    "DUMMY_PROMPT = \"Hello\"\n",
    "REAL_PROMPT = \"Explain the concept of machine learning in detail, including its types, applications, and future prospects.\"\n",
    "\n",
    "# Models to test - Testing deepseek-r1 models\n",
    "models = [\n",
    "    \"deepseek-r1:70b\",\n",
    "    \"deepseek-r1:32b\"\n",
    "]\n",
    "\n",
    "def test_model_on_hardware(model_name, server_name, server_config):\n",
    "    \"\"\"Test a specific model on a specific hardware (GPU or CPU)\"\"\"\n",
    "    hardware_type = server_config['type']\n",
    "    hardware_name = server_config['hardware']\n",
    "    \n",
    "    print(f\"\\n🔄 Testing {model_name} on {hardware_name} ({hardware_type})\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Dummy request to load model\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": DUMMY_PROMPT,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        if hardware_type == \"GPU\":\n",
    "            print(f\"   📥 Loading model into {hardware_name} VRAM...\")\n",
    "        else:\n",
    "            print(f\"   📥 Loading model into {hardware_name} memory...\")\n",
    "            \n",
    "        dummy_start = time.time()\n",
    "        dummy_response = requests.post(server_config['url'], json=payload, timeout=300)  # Longer timeout for 70B model\n",
    "        dummy_end = time.time()\n",
    "        \n",
    "        if dummy_response.status_code != 200:\n",
    "            print(f\"   ❌ Dummy request failed: {dummy_response.status_code}\")\n",
    "            print(f\"   Response: {dummy_response.text[:200]}...\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"   ✅ Model loaded in {dummy_end - dummy_start:.2f}s\")\n",
    "        \n",
    "        # 2. Real request to measure performance\n",
    "        payload[\"prompt\"] = REAL_PROMPT\n",
    "        \n",
    "        print(\"   🚀 Measuring inference performance...\")\n",
    "        start_time = time.time()\n",
    "        response = requests.post(server_config['url'], json=payload, timeout=3600)  # Extended timeout for 70B model (1 hour)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            total_time = end_time - start_time\n",
    "            \n",
    "            # Extract metrics\n",
    "            response_text = data.get(\"response\", \"\")\n",
    "            words_generated = len(response_text.split())\n",
    "            chars_generated = len(response_text)\n",
    "            words_per_second = words_generated / total_time if total_time > 0 else 0\n",
    "            \n",
    "            # Ollama-specific metrics\n",
    "            ollama_total_duration = data.get(\"total_duration\", 0) / 1e9\n",
    "            ollama_load_duration = data.get(\"load_duration\", 0) / 1e9\n",
    "            eval_count = data.get(\"eval_count\", 0)\n",
    "            eval_duration = data.get(\"eval_duration\", 0) / 1e9\n",
    "            eval_tokens_per_sec = eval_count / eval_duration if eval_duration > 0 else 0\n",
    "            prompt_eval_count = data.get(\"prompt_eval_count\", 0)\n",
    "            prompt_eval_duration = data.get(\"prompt_eval_duration\", 0) / 1e9\n",
    "            \n",
    "            print(f\"   ✅ Results for {hardware_name}:\")\n",
    "            print(f\"      ⏱️  Response Time: {total_time:.2f}s\")\n",
    "            print(f\"      ⚡ Speed: {words_per_second:.2f} words/sec\")\n",
    "            print(f\"      🏃 Ollama Tokens/sec: {eval_tokens_per_sec:.2f}\")\n",
    "            print(f\"      📝 Words Generated: {words_generated}\")\n",
    "            \n",
    "            return {\n",
    "                \"model\": model_name,\n",
    "                \"server\": server_name,\n",
    "                \"hardware\": hardware_name,\n",
    "                \"hardware_type\": hardware_type,\n",
    "                \"memory\": server_config['memory'],\n",
    "                \"cores\": server_config.get('cores', 'N/A'),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"total_time\": total_time,\n",
    "                \"words_generated\": words_generated,\n",
    "                \"words_per_second\": words_per_second,\n",
    "                \"chars_generated\": chars_generated,\n",
    "                \"loading_time\": dummy_end - dummy_start,\n",
    "                \"ollama_total_duration\": ollama_total_duration,\n",
    "                \"ollama_load_duration\": ollama_load_duration,\n",
    "                \"ollama_eval_count\": eval_count,\n",
    "                \"ollama_eval_duration\": eval_duration,\n",
    "                \"ollama_eval_tokens_per_sec\": eval_tokens_per_sec,\n",
    "                \"prompt_eval_count\": prompt_eval_count,\n",
    "                \"prompt_eval_duration\": prompt_eval_duration,\n",
    "                \"response_preview\": response_text[:200] + \"...\" if len(response_text) > 200 else response_text\n",
    "            }\n",
    "        else:\n",
    "            print(f\"   ❌ Error: {response.status_code}\")\n",
    "            print(f\"   Response: {response.text[:200]}...\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"   ❌ Timeout error on {hardware_name}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error on {hardware_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_extended_comparison():\n",
    "    \"\"\"Run performance comparison across GPUs and CPUs\"\"\"\n",
    "    print(\"🚀 DEEPSEEK-R1 MODELS PERFORMANCE COMPARISON\")\n",
    "    print(\"🎯 A40 GPU vs Contabo Server Analysis (70B & 32B)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_results = []\n",
    "    total_start = time.time()\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🧠 TESTING MODEL: {model}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model_results = []\n",
    "        \n",
    "        # Test on each server/hardware\n",
    "        for server_name, server_config in servers.items():\n",
    "            result = test_model_on_hardware(model, server_name, server_config)\n",
    "            if result:\n",
    "                all_results.append(result)\n",
    "                model_results.append(result)\n",
    "        \n",
    "        # Quick comparison for this model\n",
    "        if len(model_results) >= 2:\n",
    "            print(f\"\\n   📊 Quick Performance Overview for {model}:\")\n",
    "            \n",
    "            # Sort by performance\n",
    "            sorted_results = sorted(model_results, key=lambda x: x['words_per_second'], reverse=True)\n",
    "            \n",
    "            for i, result in enumerate(sorted_results):\n",
    "                rank_emoji = [\"🥇\", \"🥈\", \"🥉\", \"4️⃣\", \"5️⃣\"][min(i, 4)]\n",
    "                print(f\"      {rank_emoji} {result['hardware']} ({result['hardware_type']}): {result['words_per_second']:.2f} words/sec | {result['loading_time']:.2f}s loading\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    total_end = time.time()\n",
    "    \n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        \n",
    "        print(f\"\\n📊 DEEPSEEK-R1 MODELS A40 vs CONTABO COMPARISON\")\n",
    "        print(f\"⏱️  Total Test Time: {total_end - total_start:.2f}s\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Create comparison table\n",
    "        comparison_cols = ['model', 'hardware', 'hardware_type', 'total_time', 'words_per_second', \n",
    "                          'ollama_eval_tokens_per_sec', 'words_generated', 'loading_time']\n",
    "        comparison_df = df[comparison_cols].round(2)\n",
    "        comparison_df.columns = ['Model', 'Hardware', 'Type', 'Response Time (s)', 'Words/sec', \n",
    "                                'Tokens/sec', 'Words Generated', 'Loading Time (s)']\n",
    "        \n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"❌ No successful tests completed\")\n",
    "        return None\n",
    "\n",
    "# Run the extended comparison\n",
    "print(\"Starting DeepSeek-R1 Models Performance Comparison...\")\n",
    "extended_results = run_extended_comparison()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive GPU vs CPU Performance Analysis\n",
    "\n",
    "def analyze_gpu_vs_cpu_performance(df):\n",
    "    \"\"\"Provide detailed analysis comparing GPU vs CPU performance\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n🔬 COMPREHENSIVE GPU vs CPU PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Separate results by hardware type\n",
    "    gpu_results = df[df['hardware_type'] == 'GPU']\n",
    "    cpu_results = df[df['hardware_type'] == 'CPU']\n",
    "    \n",
    "    if gpu_results.empty and cpu_results.empty:\n",
    "        print(\"No results found for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Model sizes for efficiency calculations\n",
    "    model_sizes = {\n",
    "        \"deepseek-r1:70b\": 45,  # Approximate size for 70B model\n",
    "        \"deepseek-r1:32b\": 19,  # Approximate size for 32B model\n",
    "        # \"qwen3:32b\": 20,\n",
    "        # \"gemma3:27b\": 17, \n",
    "        # \"deepseek-r1:32b-qwen-distill-q4_K_M\": 19\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📊 HARDWARE TYPE PERFORMANCE OVERVIEW\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    comparison_summary = []\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n🧠 {model} (Model Size: {model_sizes.get(model, 'Unknown')} GB)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        model_gpu_results = gpu_results[gpu_results['model'] == model]\n",
    "        model_cpu_results = cpu_results[cpu_results['model'] == model]\n",
    "        \n",
    "        # GPU Performance Summary\n",
    "        if not model_gpu_results.empty:\n",
    "            print(\"   🎮 GPU Performance:\")\n",
    "            for _, gpu_row in model_gpu_results.iterrows():\n",
    "                print(f\"      • {gpu_row['hardware']}: {gpu_row['words_per_second']:.2f} words/sec | {gpu_row['loading_time']:.2f}s loading\")\n",
    "            \n",
    "            best_gpu = model_gpu_results.loc[model_gpu_results['words_per_second'].idxmax()]\n",
    "            avg_gpu_speed = model_gpu_results['words_per_second'].mean()\n",
    "            avg_gpu_loading = model_gpu_results['loading_time'].mean()\n",
    "        else:\n",
    "            print(\"   🎮 GPU Performance: No GPU results available\")\n",
    "            best_gpu = None\n",
    "            avg_gpu_speed = 0\n",
    "            avg_gpu_loading = 0\n",
    "        \n",
    "        # CPU Performance Summary\n",
    "        if not model_cpu_results.empty:\n",
    "            print(\"   🖥️  CPU Performance:\")\n",
    "            for _, cpu_row in model_cpu_results.iterrows():\n",
    "                print(f\"      • {cpu_row['hardware']}: {cpu_row['words_per_second']:.2f} words/sec | {cpu_row['loading_time']:.2f}s loading\")\n",
    "            \n",
    "            best_cpu = model_cpu_results.loc[model_cpu_results['words_per_second'].idxmax()]\n",
    "            avg_cpu_speed = model_cpu_results['words_per_second'].mean()\n",
    "            avg_cpu_loading = model_cpu_results['loading_time'].mean()\n",
    "        else:\n",
    "            print(\"   🖥️  CPU Performance: No CPU results available\")\n",
    "            best_cpu = None\n",
    "            avg_cpu_speed = 0\n",
    "            avg_cpu_loading = 0\n",
    "        \n",
    "        # Comparison\n",
    "        if best_gpu is not None and best_cpu is not None:\n",
    "            speed_advantage = ((avg_gpu_speed - avg_cpu_speed) / avg_cpu_speed) * 100\n",
    "            loading_advantage = ((avg_cpu_loading - avg_gpu_loading) / avg_cpu_loading) * 100\n",
    "            \n",
    "            print(f\"\\n   📈 GPU vs CPU Comparison:\")\n",
    "            print(f\"      Speed Advantage (GPU): {speed_advantage:+.1f}%\")\n",
    "            print(f\"      Loading Advantage (GPU): {loading_advantage:+.1f}%\")\n",
    "            print(f\"      Best GPU: {best_gpu['hardware']} ({best_gpu['words_per_second']:.2f} words/sec)\")\n",
    "            print(f\"      Best CPU: {best_cpu['hardware']} ({best_cpu['words_per_second']:.2f} words/sec)\")\n",
    "            \n",
    "            comparison_summary.append({\n",
    "                'model': model,\n",
    "                'model_size_gb': model_sizes.get(model, 0),\n",
    "                'best_gpu_hardware': best_gpu['hardware'],\n",
    "                'best_gpu_speed': best_gpu['words_per_second'],\n",
    "                'best_gpu_loading': best_gpu['loading_time'],\n",
    "                'avg_gpu_speed': avg_gpu_speed,\n",
    "                'avg_gpu_loading': avg_gpu_loading,\n",
    "                'best_cpu_hardware': best_cpu['hardware'],\n",
    "                'best_cpu_speed': best_cpu['words_per_second'],\n",
    "                'best_cpu_loading': best_cpu['loading_time'],\n",
    "                'avg_cpu_speed': avg_cpu_speed,\n",
    "                'avg_cpu_loading': avg_cpu_loading,\n",
    "                'gpu_speed_advantage_pct': speed_advantage,\n",
    "                'gpu_loading_advantage_pct': loading_advantage\n",
    "            })\n",
    "    \n",
    "    # Overall Performance Summary\n",
    "    if comparison_summary:\n",
    "        summary_df = pd.DataFrame(comparison_summary)\n",
    "        \n",
    "        print(f\"\\n🏆 OVERALL GPU vs CPU PERFORMANCE SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        avg_gpu_advantage = summary_df['gpu_speed_advantage_pct'].mean()\n",
    "        avg_loading_advantage = summary_df['gpu_loading_advantage_pct'].mean()\n",
    "        \n",
    "        print(f\"Average GPU Speed Advantage: {avg_gpu_advantage:+.1f}%\")\n",
    "        print(f\"Average GPU Loading Advantage: {avg_loading_advantage:+.1f}%\")\n",
    "        \n",
    "        # Hardware Rankings\n",
    "        print(f\"\\n🥇 PERFORMANCE RANKINGS BY HARDWARE\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Create overall ranking\n",
    "        all_hardware_performance = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            all_hardware_performance.append({\n",
    "                'hardware': row['hardware'],\n",
    "                'type': row['hardware_type'],\n",
    "                'avg_speed': df[df['hardware'] == row['hardware']]['words_per_second'].mean(),\n",
    "                'avg_loading': df[df['hardware'] == row['hardware']]['loading_time'].mean()\n",
    "            })\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        hardware_df = pd.DataFrame(all_hardware_performance).drop_duplicates('hardware').sort_values('avg_speed', ascending=False)\n",
    "        \n",
    "        for i, (_, hw_row) in enumerate(hardware_df.iterrows()):\n",
    "            rank_emoji = [\"🥇\", \"🥈\", \"🥉\", \"4️⃣\", \"5️⃣\"][min(i, 4)]\n",
    "            print(f\"{rank_emoji} {hw_row['hardware']} ({hw_row['type']}): {hw_row['avg_speed']:.2f} words/sec avg\")\n",
    "        \n",
    "        # Efficiency Analysis\n",
    "        print(f\"\\n⚡ EFFICIENCY ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        print(\"GPU Efficiency (Performance per VRAM GB):\")\n",
    "        gpu_efficiency_data = []\n",
    "        for _, row in summary_df.iterrows():\n",
    "            if 'RTX' in row['best_gpu_hardware']:\n",
    "                vram_gb = 32 if '5090' in row['best_gpu_hardware'] else 24\n",
    "                efficiency = row['best_gpu_speed'] / vram_gb\n",
    "                gpu_efficiency_data.append(f\"  • {row['model']} on {row['best_gpu_hardware']}: {efficiency:.2f} words/sec/GB\")\n",
    "            elif 'A40' in row['best_gpu_hardware']:\n",
    "                vram_gb = 48  # A40 GPU has 48GB VRAM\n",
    "                efficiency = row['best_gpu_speed'] / vram_gb\n",
    "                gpu_efficiency_data.append(f\"  • {row['model']} on {row['best_gpu_hardware']}: {efficiency:.2f} words/sec/GB\")\n",
    "        \n",
    "        for efficiency_info in gpu_efficiency_data:\n",
    "            print(efficiency_info)\n",
    "        \n",
    "        print(\"\\nCPU Efficiency (Performance characteristics):\")\n",
    "        for _, row in summary_df.iterrows():\n",
    "            print(f\"  • {row['model']} on {row['best_cpu_hardware']}: {row['best_cpu_speed']:.2f} words/sec\")\n",
    "        \n",
    "        # Use Case Recommendations\n",
    "        print(f\"\\n🎯 USE CASE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"✅ Choose GPUs when:\")\n",
    "        print(\"   • Maximum speed is critical\")\n",
    "        print(\"   • Working with large models frequently\")\n",
    "        print(\"   • Running batch inference\")\n",
    "        print(\"   • Power consumption is not a primary concern\")\n",
    "        \n",
    "        print(\"\\n✅ Choose CPUs when:\")\n",
    "        print(\"   • Cost efficiency is important\")\n",
    "        print(\"   • Power consumption is a concern\")\n",
    "        print(\"   • Running smaller models occasionally\")\n",
    "        print(\"   • Need more flexible memory allocation\")\n",
    "        \n",
    "        # Memory Utilization\n",
    "        print(f\"\\n💾 MEMORY UTILIZATION ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        largest_model_size = max(model_sizes.values())\n",
    "        print(f\"Largest tested model: {largest_model_size}GB (deepseek-r1:70b)\")\n",
    "        print(\"\\nGPU VRAM Utilization:\")\n",
    "        print(f\"  • A40 GPU (48GB): {(largest_model_size/48)*100:.1f}% utilization (70B model)\")\n",
    "        print(f\"  • A40 GPU (48GB): {(19/48)*100:.1f}% utilization (32B model)\")\n",
    "        # print(f\"  • RTX 5090 (32GB): {(largest_model_size/32)*100:.1f}% utilization\")\n",
    "        # print(f\"  • RTX 4090 (24GB): {(largest_model_size/24)*100:.1f}% utilization\")\n",
    "        print(\"\\nCPU RAM Utilization:\")\n",
    "        print(\"  • Contabo Server: Flexible allocation from system RAM\")\n",
    "        # print(\"  • AMD EPYC: Flexible allocation from system RAM\")\n",
    "        # print(\"  • AMD Threadripper: Flexible allocation from system RAM\")\n",
    "        print(\"  • Note: CPU inference can use system RAM dynamically\")\n",
    "        \n",
    "        return summary_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def save_extended_results(df, analysis_df=None):\n",
    "    \"\"\"Save extended comparison results\"\"\"\n",
    "    if df is not None:\n",
    "        # Save raw results\n",
    "        df.to_csv(\"gpu_cpu_comparison_raw.csv\", index=False)\n",
    "        print(f\"\\n💾 Raw GPU vs CPU results saved to gpu_cpu_comparison_raw.csv\")\n",
    "        \n",
    "        # Save analysis summary if available\n",
    "        if analysis_df is not None:\n",
    "            analysis_df.to_csv(\"gpu_cpu_performance_analysis.csv\", index=False)\n",
    "            print(f\"💾 Performance analysis saved to gpu_cpu_performance_analysis.csv\")\n",
    "\n",
    "# Run the comprehensive analysis\n",
    "if 'extended_results' in locals() and extended_results is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    analysis_summary = analyze_gpu_vs_cpu_performance(extended_results)\n",
    "    save_extended_results(extended_results, analysis_summary)\n",
    "else:\n",
    "    print(\"Run the extended comparison first to get results for GPU vs CPU analysis\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
